{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using CUDA!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import trees\n",
    "import vocabulary\n",
    "import nkutil\n",
    "import parse_nk\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\oscillating_rnn\\\\Dyer\\\\self-attentive-parser\\\\plots'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Path(os.getcwd()).parent.parent.parent) + '\\Dyer\\self-attentive-parser\\plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def gen_wordlist(len_wordlist=60):\n",
    "    \"\"\"Generates wordlist as opposed to sentences in the Ding data.\n",
    "\n",
    "    output structure:    \"w1 w2 w3 w4.\"\n",
    "    \"\"\"\n",
    "    full_ding = pd.read_csv(str(Path(os.getcwd()).parent) + '\\\\Data\\Ding_grammatical.csv',\n",
    "                           header=None)\n",
    "    # get all nouns\n",
    "    nouns = pd.unique(pd.concat([full_ding[1],full_ding[3]]))\n",
    "    adjectives = pd.unique(full_ding[0])\n",
    "    verbs = pd.unique(full_ding[2])\n",
    "    all_words = np.unique(np.concatenate([nouns,adjectives,verbs]))\n",
    "    word_salad_seqs = []\n",
    "    for i in range(60):\n",
    "        word_salad = np.random.choice(all_words,size=4,replace=False)\n",
    "        word_salad_seq = ' '.join(word_salad) #+ '.'\n",
    "        word_salad_seqs.append(word_salad_seq)\n",
    "    jabberwocky_seqs = []\n",
    "    for i in range(60):\n",
    "        jabberwocky = np.random.choice(adjectives,size=1) + ' ' + np.random.choice(nouns,size=1) + ' ' + \\\n",
    "            np.random.choice(verbs,size=1) + ' ' + np.random.choice(nouns,size=1)# + '.'\n",
    "        jabberwocky_seqs.append(list(jabberwocky))\n",
    "    jabberwocky_seqs = [i[0] for i in jabberwocky_seqs]\n",
    "    np_seqs = []\n",
    "    for i in range(60):\n",
    "        np_seq = np.random.choice(adjectives,size=1) + ' ' + np.random.choice(nouns,size=1) + ' ' + \\\n",
    "            np.random.choice(adjectives,size=1) + ' ' + np.random.choice(nouns,size=1)# + '.'\n",
    "        np_seqs.append(list(np_seq))\n",
    "    np_seqs = [i[0] for i in np_seqs]\n",
    "    vp_seqs = []\n",
    "    for i in range(60):\n",
    "        vp_seq = np.random.choice(verbs,size=1) + ' ' + np.random.choice(nouns,size=1) + ' ' + \\\n",
    "            np.random.choice(verbs,size=1) + ' ' + np.random.choice(nouns,size=1) #+ '.'\n",
    "        vp_seqs.append(list(vp_seq))\n",
    "    vp_seqs = [i[0] for i in vp_seqs]\n",
    "    # keep random pattern - say - verb, noun, noun, adjective\n",
    "    random_patterns = []\n",
    "    for i in range(60):\n",
    "        random_pattern = np.random.choice(verbs,size=1) + ' ' + np.random.choice(nouns,size=1) + ' ' + \\\n",
    "            np.random.choice(nouns,size=1) + ' ' + np.random.choice(adjectives,size=1) #+ '.'\n",
    "        random_patterns.append(list(random_pattern))\n",
    "    random_patterns = [i[0] for i in random_patterns]\n",
    "\n",
    "    return [word_salad_seqs,jabberwocky_seqs,np_seqs,vp_seqs,random_patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\rnn-oscillations\\\\berkeley_neural_parser\\\\Data\\\\en_charlstm_dev.93.61.pt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Path(os.getcwd()).parent) + \"\\\\Data\\\\en_charlstm_dev.93.61.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sentences : \n",
    "sentences = []\n",
    "with open(str(Path(os.getcwd()).parent) + '\\\\Data\\\\Ding_grammatical.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        sentences.append(' '.join(row))\n",
    "        #sentences[-1] = sentences[-1] + '.'\n",
    "        #print(sentences[-1])\n",
    "temp = torch.load(str(Path(os.getcwd()).parent) + \"\\\\Data\\\\en_charlstm_dev.93.61.pt\")\n",
    "parser = parse_nk.NKChartParser.from_spec(temp['spec'], temp['state_dict'])\n",
    "\n",
    "sentences = ' '.join(sentences)\n",
    "sentences = sentences.split()\n",
    "\n",
    "sjnv = gen_wordlist()\n",
    "sjnv = [subitem for sublist in sjnv for subitem in sublist]\n",
    "sjnv = ' '.join(sjnv)\n",
    "sjnv = sjnv.split()\n",
    "# sentences_all = sentences + sjnv\n",
    "# sentences_all = sentences, sjnv[0:240] (word salad), sjnv[240:480] (jabberwocky), sjnv[480:720] (np), sjnv[720:] (vp)\n",
    "sentences_all = sentences + sjnv\n",
    "\n",
    "if 'UNK' in parser.tag_vocab.indices:\n",
    "    dummy_tag = 'UNK'\n",
    "else:\n",
    "    dummy_tag = parser.tag_vocab.value(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(seq_batch):\n",
    "    outputs_0= []\n",
    "    def hook_0(module, input, output):\n",
    "        outputs_0.append(output)\n",
    "    outputs_1 = []\n",
    "    def hook_1(module, input, output):\n",
    "        outputs_1.append(output)\n",
    "    outputs_2 = []\n",
    "    def hook_2(module, input, output):\n",
    "        outputs_2.append(output)\n",
    "    outputs_3 = []\n",
    "    def hook_3(module, input, output):\n",
    "        outputs_3.append(output)\n",
    "    outputs_4 = []\n",
    "    def hook_4(module, input, output):\n",
    "        outputs_4.append(output)\n",
    "    outputs_5 = []\n",
    "    def hook_5(module, input, output):\n",
    "        outputs_5.append(output)\n",
    "    outputs_6 = []\n",
    "    def hook_6(module, input, output):\n",
    "        outputs_6.append(output)\n",
    "    outputs_7 = []\n",
    "    def hook_7(module, input, output):\n",
    "        outputs_7.append(output)\n",
    "\n",
    "\n",
    "\n",
    "    #original model = parser\n",
    "    #parser.char_encoder.lstm.register_forward_hook(hook)\n",
    "    #parser.f_label[3].register_forward_hook(hook)\n",
    "    #parser.encoder.ff_0.layer_norm.register_forward_hook(hook_0)\n",
    "    parser.encoder.ff_1.layer_norm.register_forward_hook(hook_0)\n",
    "    parser.encoder.ff_1.layer_norm.register_forward_hook(hook_1)\n",
    "    parser.encoder.ff_2.layer_norm.register_forward_hook(hook_2)\n",
    "    parser.encoder.ff_3.layer_norm.register_forward_hook(hook_3)\n",
    "    parser.encoder.ff_4.layer_norm.register_forward_hook(hook_4)\n",
    "    parser.encoder.ff_5.layer_norm.register_forward_hook(hook_5)\n",
    "    parser.encoder.ff_6.layer_norm.register_forward_hook(hook_6)\n",
    "    parser.encoder.ff_7.layer_norm.register_forward_hook(hook_7)\n",
    "    out, _ = parser.parse_batch(seq_batch)\n",
    "    del _\n",
    "\n",
    "    temp_acts = outputs_0[0].numpy() + outputs_1[0].numpy() + outputs_2[0].numpy() + outputs_3[0].numpy() + \\\n",
    "        outputs_4[0].numpy() + outputs_5[0].numpy() + outputs_6[0].numpy() + outputs_7[0].numpy()\n",
    "    # drop first and last one\n",
    "    temp_acts = temp_acts[1:-1,:]\n",
    "    return temp_acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power spectra function\n",
    "def power_spectra(w2vec_array, sampling_rate=16):\n",
    "    \"\"\"Computes power spectra using the Discrete Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        w2vec_array: arrays of shape (n_sentences*time_steps, n_units)\n",
    "            representing hidden layer activations in response to each word of the\n",
    "            concatenated sequence of sentences.\n",
    "        sample_rate: number of measures (outputs of the model) per second.\n",
    "\n",
    "    Returns:\n",
    "        Mean power spectra and frequency axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate each unit and concatenate across words to form 1 vector per unit\n",
    "    unit_vectors = np.transpose(w2vec_array)\n",
    "\n",
    "    # Frequency domain\n",
    "    # num_samples is just the number of data points for each unit\n",
    "    num_samples = unit_vectors.shape[1]\n",
    "    freq = np.fft.rfftfreq(num_samples, d=1./sampling_rate)\n",
    "    print ('freq ' ,freq.shape)\n",
    "    # Calculate the FFT and power spectra for each unit\n",
    "    units_ps = []\n",
    "    for vector in unit_vectors:\n",
    "        ft_unit = np.fft.rfft(vector)  # fft\n",
    "        ps_uni = np.abs(ft_unit) ** 2  # power spectrum\n",
    "        units_ps.append(ps_uni)\n",
    "\n",
    "    # Average power spectra over units\n",
    "    unit_ps = np.array(unit_ps)\n",
    "    mean_ps = np.mean(units_ps, axis=0)\n",
    "    print ('mean_ps',mean_ps.shape)\n",
    "    return freq, mean_ps, unit_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(file_path, freq, power, act,sampling_rate=16,n_samples=60,title='Input stimuli'):\n",
    "    \"\"\"Plot all the results of one condition\"\"\"\n",
    "\n",
    "    # Time domain parameters\n",
    "    sampling_interval = 1.0/sampling_rate  # sampling interval\n",
    "    t = np.arange(0, n_samples, sampling_interval)  # time vector\n",
    "    # Build plot\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(t, act, linewidth=1.0)\n",
    "    # Axis labels\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Amplitude')\n",
    "    #until_4 = np.where(freq==4)[0][0]\n",
    "    #print('Power difference ')\n",
    "    ax[1].plot(freq[1:], power[1:], 'r', linewidth=1.0)\n",
    "    ax[1].set_xlabel('Freq (Hz)')\n",
    "    #ax[1].set_ylim([-1000, 9000])\n",
    "    ax[1].set_ylabel('Power (db)')\n",
    "    # Adjusts subplot\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    # Save\n",
    "    fig.savefig(file_path+'.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_zscores(vals,window=10):\n",
    "    scores = np.zeros(vals.shape)\n",
    "    seq_length = vals.shape[0]\n",
    "    for i in range(seq_length):\n",
    "        # before window length is reached\n",
    "        if i < window:\n",
    "            scores[i] = vals[i] - np.concatenate([vals[i+1:i+1+window],vals[0:i]]).mean()\n",
    "        # less than window length remaining\n",
    "        elif (i + window) > seq_length:\n",
    "            scores[i] = vals[i] - np.concatenate([vals[i-window:i],vals[i+1:]]).mean()\n",
    "        # after window length is reached\n",
    "        else:\n",
    "            scores[i] = vals[i] - np.concatenate([vals[i+1:i+1+window],vals[i-window:i]]).mean()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_plot(activations,plot_title,file_path,condition):\n",
    "    freq, mean_power, unit_ps = power_spectra(activations,sampling_rate=4)\n",
    "    mean_power = windowed_zscores(mean_power,window=10)\n",
    "    # write frequency and mean power and activations to another folder\n",
    "    data_storage_path = str(Path(os.getcwd()).parent) + '\\\\Data\\\\data-vals\\\\' + condition + str('\\\\')\n",
    "    np.save(data_storage_path + 'freq.npy',freq)\n",
    "    np.save(data_storage_path + 'unit_power.npy',unit_ps)\n",
    "    np.save(data_storage_path + 'activations.npy',np.mean(activations,axis=1))\n",
    "    plot_results(file_path, \n",
    "                 freq, mean_power, act=np.mean(activations,axis=1),\n",
    "                 sampling_rate=4,n_samples=60,title=plot_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1024)\n",
      "freq  (121,)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'unit_ps' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ef0937dc541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     compute_and_plot(activations=activations,plot_title=info_dict[i][1],file_path=info_dict[i][0],\n\u001b[1;32m---> 21\u001b[1;33m                     condition=data_names[i])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-1c8e0c9b4d81>\u001b[0m in \u001b[0;36mcompute_and_plot\u001b[1;34m(activations, plot_title, file_path, condition)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_and_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot_title\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpower_spectra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmean_power\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindowed_zscores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_power\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# write frequency and mean power and activations to another folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata_storage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\Data\\\\data-vals\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3447bd9f2b6c>\u001b[0m in \u001b[0;36mpower_spectra\u001b[1;34m(w2vec_array, sampling_rate)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Average power spectra over units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0munit_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit_ps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mmean_ps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits_ps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'mean_ps'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_ps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'unit_ps' referenced before assignment"
     ]
    }
   ],
   "source": [
    "info_dict = {0:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\regular_sentences',\n",
    "                'Grammatical Sentences'],\n",
    "             1:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\word_salad',\n",
    "                'Word Salad'],\n",
    "             2:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\jabberwocky',\n",
    "                'Jabberwocky'],\n",
    "             3:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\noun_phrase',\n",
    "                'Noun Phrase'],\n",
    "             4:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\verb_phrase',\n",
    "                'Verb Phrase'],\n",
    "             5:[str(Path(os.getcwd()).parent) + '\\\\Data\\\\plots-parser-smoothed\\\\random_pattern',\n",
    "                'Random Pattern - v,n,n,a']}\n",
    "data_names = ['valid_sentences','word_salad','jabberwocky','noun_phrases','verb_phrases','random_pattern']\n",
    "\n",
    "for i in range(6):\n",
    "    subbatch_sentences = sentences_all[i*240:240*(i+1)]\n",
    "    subbatch_sentences = [[(dummy_tag, word) for word in sentences_all[i*240:240*(i+1)]]]\n",
    "    activations = process_sequences(subbatch_sentences)\n",
    "    print(activations.shape)\n",
    "    compute_and_plot(activations=activations,plot_title=info_dict[i][1],file_path=info_dict[i][0],\n",
    "                    condition=data_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
